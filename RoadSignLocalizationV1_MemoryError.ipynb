{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0b6wP0gl5Lkq2mKj5cmLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhello007/PersonalProjects-DeepLearning/blob/main/RoadSignLocalizationV1_MemoryError.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ375HIhjnNh",
        "outputId": "3e824794-d759-4799-881a-692a722096a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/root')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CvGD4RUM8er",
        "outputId": "361e771d-668a-48f0-8ad7-a6ebd43f10e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.bashrc',\n",
              " '.profile',\n",
              " '.jupyter',\n",
              " '.local',\n",
              " '.tmux.conf',\n",
              " '.cache',\n",
              " '.ipython',\n",
              " '.config',\n",
              " '.npm',\n",
              " '.wget-hsts',\n",
              " '.keras',\n",
              " '.launchpadlib']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zeQekMvgf1zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle #create a directory\n"
      ],
      "metadata": {
        "id": "69H-SwJ2luGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/kaggle.json' ~/.kaggle/ #copy the json file to this directory"
      ],
      "metadata": {
        "id": "MGcin-aDl2xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json #giving permission to read and write"
      ],
      "metadata": {
        "id": "lMVkU4zcme2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download andrewmvd/road-sign-detection"
      ],
      "metadata": {
        "id": "Fvxvu17noLU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/root/.kaggle'\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "id": "Q0ioDunnsFIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip road-sign-detection.zip\n"
      ],
      "metadata": {
        "id": "xTXhYkQctIM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as xt\n",
        "from bs4 import BeautifulSoup\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "Suc17SEatOPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xml_eg = '/content/annotations/road0.xml'"
      ],
      "metadata": {
        "id": "oANIpkuIwHfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(xml_eg,'r') as f:\n",
        "  data = f.read()\n",
        "  content = BeautifulSoup(data,'xml')\n",
        "\n",
        "content"
      ],
      "metadata": {
        "id": "6paIgnR8OTB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = sorted(glob('/content/annotations/*.xml')) #this is the annotations path\n",
        "path"
      ],
      "metadata": {
        "id": "ATkZZEt5aOsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = sorted(glob('/content/images/*.png')) #this is the images path\n",
        "images_path"
      ],
      "metadata": {
        "id": "N2tSRAKhac4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Code below basically compares if the annotation paths and the images path are arranged in the same order in their respective lists."
      ],
      "metadata": {
        "id": "gpNF-YB_sASR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wrote this to make sure every image has its corresponding bbox values in the correct values\n",
        "num = len(images_path)\n",
        "a=0\n",
        "for i in range(num):\n",
        "  pat = path[i]\n",
        "  img = images_path[i]\n",
        "  filename1 = pat.split('/')[-1] #got the last part of path\n",
        "  filename2 = img.split('/')[-1]\n",
        "\n",
        "  key1 = filename1.split('.')[0] #got the 0th index sep by .\n",
        "  key2 = filename2.split('.')[0]\n",
        "  if(key1!=key2):\n",
        "    print(\"not the same\")\n",
        "    a+=1"
      ],
      "metadata": {
        "id": "2qSx2z68grQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a #this is the number of annotations and imgs which do not match"
      ],
      "metadata": {
        "id": "Z0d0PdqgjRRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We're extracting every bbox value and also extracting the labels into a list y\n",
        "path = sorted(glob('/content/annotations/*.xml'))\n",
        "y = []\n",
        "\n",
        "for file in path:\n",
        "  label =0\n",
        "  info  = xt.parse(file)\n",
        "  root = info.getroot()\n",
        "  obj = root.find('object')\n",
        "\n",
        "  label_obj  = obj.find('name').text\n",
        "  if \"trafficlight\" in label_obj:\n",
        "    label = 1\n",
        "  elif \"stop\" in label_obj:\n",
        "    label = 2\n",
        "  elif \"speedlimit\" in label_obj:\n",
        "    label = 3\n",
        "  elif \"crosswalks\" in label_obj:\n",
        "    label = 4\n",
        "\n",
        "  bndbox = obj.find('bndbox')\n",
        "  xmin = (bndbox.find('xmin').text)\n",
        "  xmax = (bndbox.find('xmax').text)\n",
        "  ymin = (bndbox.find('ymin').text)\n",
        "  ymax = (bndbox.find('ymax').text)\n",
        "\n",
        "  y.append([int(label),int(xmin),int(ymin),int(xmax),int(ymax)])"
      ],
      "metadata": {
        "id": "Z07X_wf9OdmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "id": "04ER2qv3_TCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[513]"
      ],
      "metadata": {
        "id": "QmBdglrSs_uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "\n",
        "images_path = sorted(glob('/content/images/*.png'))\n",
        "\n",
        "for image in images_path:\n",
        "  img = cv2.imread(image) #this img is a numpy array\n",
        "  x.append(img) #since x is a list , its a lotta numpy arrays in the list\n"
      ],
      "metadata": {
        "id": "zkbPVHlCQvuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(x[0])"
      ],
      "metadata": {
        "id": "101TU1ljtfeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B2IrF6AvuDYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "id": "OBUF4jtytfiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image = Image.open(images_path[0]) #you can open the image like this"
      ],
      "metadata": {
        "id": "6gnNtfpaVmnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(image)"
      ],
      "metadata": {
        "id": "JQJKxxPxWQpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.size #width, height"
      ],
      "metadata": {
        "id": "hhIhqnBWu1Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the images and the targets:containing the class and its bboxes, we have to resize the images and their respective bounding boxes."
      ],
      "metadata": {
        "id": "5FhoD0IuWC9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x[0])\n"
      ],
      "metadata": {
        "id": "v9C7GjXAGxUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "bbox = y[513][12:]\n",
        "img_eg = x[513]\n",
        "fig = px.imshow(img_eg)\n",
        "fig.add_shape(type='rect', x0=bbox[0],x1=bbox[2],y0=bbox[1],y1=bbox[3],xref='x',yref='y',line_color='cyan')\n",
        "print(bbox)"
      ],
      "metadata": {
        "id": "AA_zYd8bN3u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y[513][0] #its class/label"
      ],
      "metadata": {
        "id": "0sqRyywYYvGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets make a new directory for resized images\n",
        "\n",
        "os.mkdir('resized_imgs/')"
      ],
      "metadata": {
        "id": "mQT5JM_SAFeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#playing around with lists\n",
        "bblist = []\n",
        "for subset in y:\n",
        "  bbvalues = subset[1:5]\n",
        "  bblist.append(bbvalues)\n",
        "\n",
        "bblist"
      ],
      "metadata": {
        "id": "oK_OwMrsMC41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eg = '/content/images/road1.png'\n",
        "img = cv2.imread(eg) #opencv opens images in BGR format and not RGB. Here it doesnt really need to be converted to an rgb image.\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "mx5U5Bg-K6kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(eg) #opening the same image with Image.open() and showing its resized value.\n",
        "img2 = img.resize((800,800))\n",
        "plt.imshow(img2)"
      ],
      "metadata": {
        "id": "LbFanrucMSGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are resizing the images one by one, saving the resized image, resizing their bboxes and saving them in a resized_bblist[]"
      ],
      "metadata": {
        "id": "0hJgwKSrWuMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "width = 400\n",
        "height = 400\n",
        "output_folder = '/content/resized_imgs'\n",
        "resized_bblist =  []\n",
        "i =0\n",
        "for image in images_path:\n",
        "  img = Image.open(image)\n",
        "  original_width, original_height = img.size\n",
        "\n",
        "  ratio_width = width / original_width\n",
        "  ratio_height = height / original_height\n",
        "  #resized the image\n",
        "  resized_image = img.resize((width,height))\n",
        "\n",
        "  #save the image\n",
        "  filename = os.path.basename(image)\n",
        "  output_path = os.path.join(output_folder,filename)\n",
        "  resized_image.save(output_path)\n",
        "\n",
        "  #that images' bbox\n",
        "  xmin = bblist[i][0]\n",
        "  ymin = bblist[i][1]\n",
        "  xmax = bblist[i][2]\n",
        "  ymax = bblist[i][3]\n",
        "  i+=1\n",
        "  nxmin = int(xmin * ratio_width)\n",
        "  nymin = int(ymin * ratio_height)\n",
        "  nxmax = int(xmax * ratio_width)\n",
        "  nymax = int(ymax * ratio_height)\n",
        "  resized_bblist.append([int(nxmin), int(nymin), int(nxmax), int(nymax)])\n"
      ],
      "metadata": {
        "id": "sMrl2seAWm6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resized_bblist[0]"
      ],
      "metadata": {
        "id": "GQyVoJP2R7EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting the resized_images and reading them in using opencv\n",
        "resized_images = []\n",
        "resized_path = sorted(glob('/content/resized_imgs/*.png'))\n",
        "\n",
        "for image in resized_path:\n",
        "  img = cv2.imread(image)\n",
        "  resized_images.append(img)"
      ],
      "metadata": {
        "id": "Y-_dt7L9ULNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(resized_images[0])"
      ],
      "metadata": {
        "id": "IfOE1cPUwaWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#viewing the resized image and its bbox\n",
        "bbox = resized_bblist[210]\n",
        "img = resized_images[210]\n",
        "fig = px.imshow(img)\n",
        "fig.add_shape(type='rect',x0=bbox[0],x1=bbox[2],y0=bbox[1],y1=bbox[3],xref='x',yref='y',line_color='cyan')"
      ],
      "metadata": {
        "id": "r30c2NxaKmAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a function to take the class label from y and joining it with the new resized bbox values to merge into a new list.\n",
        "def merge_lists(list1,list2):\n",
        "  merged_list = [[l1[0]] +l2 for l1,l2, in zip(list1,list2)]\n",
        "  return merged_list"
      ],
      "metadata": {
        "id": "w8t0m98UHx5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_list = merge_lists(y,resized_bblist)"
      ],
      "metadata": {
        "id": "s0vL_XgxXH8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_list[210]"
      ],
      "metadata": {
        "id": "coIJMNKXUWoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_list)"
      ],
      "metadata": {
        "id": "K_GVl6kVXWHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the resized images and the bboxes to a numpy array\n",
        "resized_images = np.array(resized_images)\n",
        "merged_lab_bbox = np.array(merged_list) #this has labels and bbox"
      ],
      "metadata": {
        "id": "nbJ-9zAYVR1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle the clean dataset\n",
        "resized_images = np.random.permutation(resized_images)\n",
        "merged_lab_bbox = np.random.permutation(merged_list)"
      ],
      "metadata": {
        "id": "h6Lx7JOtVash"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_lab_bbox[0]"
      ],
      "metadata": {
        "id": "ehbGCwtOU0L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resized_images[2].shape , merged_lab_bbox[2].shape"
      ],
      "metadata": {
        "id": "VcPQJWe1VgpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT, we dont have an X_test cause we can test on a new image. we really need val dataset.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_val,y_train,y_val = train_test_split(resized_images,merged_lab_bbox,test_size=0.2)"
      ],
      "metadata": {
        "id": "ahziGfKtVh-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "aOUUdDwTYpVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "F0KcpZv8SPTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape #validation set"
      ],
      "metadata": {
        "id": "t693DMzHwrkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "pI_5EgmWZmMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the Xtrain and Xval values\n",
        "X_train = X_train/255.\n",
        "X_val = X_val/255."
      ],
      "metadata": {
        "id": "OhNLzKKSaO46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets Start Training the Model"
      ],
      "metadata": {
        "id": "Hnot01xMxx7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ],
      "metadata": {
        "id": "JK75jjL9Tbpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5 #4 + 1 background\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "#replace the pretrained head with a new one to perform localization and classification\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n"
      ],
      "metadata": {
        "id": "LNbCOxHGT02j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By subclassing torch.utils.data.Dataset, you can create a dataset object that can be used with PyTorch's data loading utilities, such as torch.utils.data.DataLoader, to efficiently load and process data during training or inference. A custom dataset typically represents a collection of data samples and their corresponding labels or annotations."
      ],
      "metadata": {
        "id": "p8xEuG2fbo7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert numpy images to a pytorch tensor\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "num_train_images = X_train.shape[0]\n",
        "num_val_images = X_val.shape[0]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "tensor_train_images = []\n",
        "tensor_val_images = []\n",
        "for i in range(num_train_images):\n",
        "  image = X_train[i]\n",
        "  preproc_img = transform(image)\n",
        "  tensor_train_images.append(preproc_img)\n",
        "\n",
        "tensor_train_images = torch.stack(tensor_train_images)\n",
        "\n",
        "#Now for val images\n",
        "for i in range(num_val_images):\n",
        "  image = X_val[i]\n",
        "  preproc_img = transform(image)\n",
        "  tensor_val_images.append(preproc_img)\n",
        "\n",
        "tensor_val_images = torch.stack(tensor_val_images)"
      ],
      "metadata": {
        "id": "JWk0TfQJ3GfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_train_images.shape ,  tensor_val_images.shape"
      ],
      "metadata": {
        "id": "bjewDUsCZNzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now write a function to convert a bounding box to a pytorch tensor\n",
        "def convert_bbox_to_tensor(bbox):\n",
        "  x1,y1,x2,y2 = bbox\n",
        "  bbox_tensor = torch.tensor([x1,y1,x2,y2], dtype=torch.float32)\n",
        "  return bbox_tensor"
      ],
      "metadata": {
        "id": "zD3zeNpuZN8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nowe use the function created above to create a pytorch bbox.\n",
        "tensor_train_bboxes = []\n",
        "tensor_val_bboxes = []\n",
        "#for train bboxes\n",
        "for i in range(num_train_images):\n",
        "    bbox = y_train[i][1:]  # Exclude the label from the bbox values\n",
        "\n",
        "    preproc_bbox = convert_bbox_to_tensor(bbox)  # Convert bbox numpy to a PyTorch tensor\n",
        "    tensor_train_bboxes.append(preproc_bbox)\n",
        "\n",
        "tensor_train_bboxes = torch.stack(tensor_train_bboxes)\n",
        "\n",
        "\n",
        "#now for validation bboxes\n",
        "for i in range(num_val_images):\n",
        "    bbox = y_val[i][1:]  # Exclude the label from the bbox values\n",
        "\n",
        "    preproc_bbox = convert_bbox_to_tensor(bbox)  # Convert bbox numpy to a PyTorch tensor\n",
        "    tensor_val_bboxes.append(preproc_bbox)\n",
        "\n",
        "tensor_val_bboxes = torch.stack(tensor_val_bboxes)"
      ],
      "metadata": {
        "id": "abH84kilZN_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a function to convert labels into pytorch tensors\n",
        "def convert_labels_to_tensor(label):\n",
        "  label_tensor = torch.tensor([label], dtype=torch.float32)\n",
        "  return label_tensor"
      ],
      "metadata": {
        "id": "PpGoaqpfZOGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_train_labels = []\n",
        "tensor_val_labels = []\n",
        "for i in range(num_train_images):\n",
        "  label = y_train[i][0]\n",
        "\n",
        "  labeled = convert_labels_to_tensor(label)\n",
        "  tensor_train_labels.append(labeled)\n",
        "\n",
        "tensor_train_labels = torch.stack(tensor_train_labels)\n",
        "\n",
        "\n",
        "#now for val labels\n",
        "for i in range(num_val_images):\n",
        "  label = y_val[i][0]\n",
        "\n",
        "  labeled = convert_labels_to_tensor(label)\n",
        "  tensor_val_labels.append(labeled)\n",
        "\n",
        "tensor_val_labels = torch.stack(tensor_val_labels)"
      ],
      "metadata": {
        "id": "jrfDTbYfZOIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create the Main Road Sign Dataset FUnciton"
      ],
      "metadata": {
        "id": "yBwGUVHfz5Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class RoadSignDataset(Dataset):\n",
        "    def __init__(self, images, bounding_boxes, labels):\n",
        "        self.images = images\n",
        "        self.bounding_boxes = bounding_boxes\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        bounding_box = self.bounding_boxes[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        return image, {'boxes': bounding_box, 'labels': label}\n"
      ],
      "metadata": {
        "id": "rPuN_LI1ZOT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all of this in data loader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "train_dataset = RoadSignDataset(tensor_train_images, tensor_train_bboxes, tensor_train_labels)\n",
        "val_dataset = RoadSignDataset(tensor_val_images, tensor_val_bboxes, tensor_val_labels)\n",
        "train_dl = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size = 16, shuffle= True)"
      ],
      "metadata": {
        "id": "lWPPjP0jZOWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl , val_dl"
      ],
      "metadata": {
        "id": "0VAQZoSTNSfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a very simple pre-trained resNet-34 model as our model. Since we have two tasks here, there are two final layers - the bounding box regression and the image classifier"
      ],
      "metadata": {
        "id": "tmzhIuYjG8AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "class BB_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BB_model, self).__init__()\n",
        "        resnet = models.resnet34(pretrained=True)\n",
        "        layers = list(resnet.children())[:8]\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
        "        self.bb = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = F.relu(x)\n",
        "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        return self.classifier(x), self.bb(x)"
      ],
      "metadata": {
        "id": "faDvwh6vjq27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = models.resnet34(pretrained=True)\n",
        "list(resnet.children())[:8]"
      ],
      "metadata": {
        "id": "2CCX9-0-jq49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the loss, we need to take into account both the classification loss and the bounding box regression loss, so we use a combination of cross entropy and L1 loss"
      ],
      "metadata": {
        "id": "qm3D-ST3HrhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BB_model()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "cuLVQQ1nD8mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(params, lr=0.006)\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "S5GxcrBxL5ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_loss(outputs, targets, lambda_coord=5.0):\n",
        "    pred_class, pred_bb = outputs\n",
        "    true_class, true_bb = targets['labels'], targets['boxes']\n",
        "\n",
        "\n",
        "    if 'labels' not in targets or 'boxes' not in targets:\n",
        "        raise ValueError(\"Targets dictionary should contain 'labels' and 'boxes'.\")\n",
        "\n",
        "\n",
        "    classification_loss = F.cross_entropy(pred_class, true_class.squeeze().long()) # Classification loss using cross-entropy\n",
        "\n",
        "\n",
        "    localization_loss = F.smooth_l1_loss(pred_bb, true_bb, reduction='mean')\n",
        "\n",
        "\n",
        "    total_loss = classification_loss + lambda_coord * localization_loss  # Total loss as a combination of class and localization los\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "RHXK_CenOHw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        total = 0\n",
        "        sum_loss = 0\n",
        "\n",
        "\n",
        "        for images, targets in train_dl:\n",
        "            images = images.cuda().float()\n",
        "            bounding_boxes = targets['boxes'].cuda().float()\n",
        "            labels = targets['labels'].cuda()\n",
        "\n",
        "            # Forward paaass and loss calc\n",
        "            outputs = model(images)\n",
        "            loss = calculate_loss(outputs, {'boxes': bounding_boxes, 'labels': labels})\n",
        "\n",
        "            # backprop and opt\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += images.size(0)\n",
        "            sum_loss += loss.item()\n",
        "\n",
        "        train_loss = sum_loss / total\n",
        "\n",
        "        # Validatn loop\n",
        "        model.eval()\n",
        "        val_total = 0\n",
        "        val_sum_loss = 0\n",
        "        val_correct = 0\n",
        "\n",
        "        for val_images, val_targets in val_dl:\n",
        "            val_images = val_images.cuda().float()\n",
        "            val_bounding_boxes = val_targets['boxes'].cuda().float()\n",
        "            val_labels = val_targets['labels'].cuda()\n",
        "\n",
        "            # Forward p and loss calcu or validation\n",
        "            val_outputs = model(val_images)\n",
        "            val_loss = calculate_loss(val_outputs, {'boxes': val_bounding_boxes, 'labels': val_labels})\n",
        "\n",
        "            # Calculate val accuracy\n",
        "            val_preds = torch.argmax(val_outputs[1], dim=1)  # Assuming label predictions are in index 1\n",
        "            val_correct += torch.sum(val_preds == val_labels.squeeze()).item()\n",
        "\n",
        "            val_total += val_images.size(0)\n",
        "            val_sum_loss += val_loss.item()\n",
        "\n",
        "        val_loss = val_sum_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch [{i+1}/{epochs}]. train_loss {train_loss:.3f} val_loss {val_loss:.3f} val_acc {val_acc:.3f}\")\n",
        "        val_loss = val_sum_loss.item()\n"
      ],
      "metadata": {
        "id": "hGclLcxAKnnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "F5nNYhKaMns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5_Vy5WgGI-qp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-agr-wggtHdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}